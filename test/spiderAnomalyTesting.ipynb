{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735bc287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import required modules\n",
    "from anomalib.data import MVTecAD\n",
    "from anomalib.deploy import ExportType\n",
    "from anomalib.engine import Engine\n",
    "from anomalib.models import Patchcore\n",
    "from anomalib.data import Folder\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd580a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def albu_adapter(aug):\n",
    "    \"\"\"Wrap an Albumentations Compose so it accepts (image) and returns a Tensor.\n",
    "       Converts PIL/Tensor to NumPy HxWxC uint8 before calling Albumentations.\n",
    "    \"\"\"\n",
    "    def _call(image):\n",
    "        # 1) Normalize input type to NumPy HxWxC\n",
    "        if isinstance(image, np.ndarray):\n",
    "            img = image\n",
    "            # If CHW, convert to HWC\n",
    "            if img.ndim == 3 and img.shape[0] in (1, 3, 4) and img.shape[2] not in (1, 3, 4):\n",
    "                img = np.transpose(img, (1, 2, 0))\n",
    "        elif isinstance(image, Image.Image):\n",
    "            img = np.array(image)  # PIL -> HWC uint8\n",
    "        elif isinstance(image, torch.Tensor):\n",
    "            arr = image.detach().cpu().numpy()\n",
    "            # Assume CHW; convert to HWC\n",
    "            if arr.ndim == 3 and arr.shape[0] in (1, 3, 4):\n",
    "                arr = np.transpose(arr, (1, 2, 0))\n",
    "            # If float in [0,1], convert to uint8 0..255 for Albumentations Normalize defaults\n",
    "            if arr.dtype != np.uint8:\n",
    "                arr = np.clip(arr, 0, 1) * 255.0\n",
    "                arr = arr.astype(np.uint8)\n",
    "            img = arr\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported image type: {type(image)}. Expected numpy, PIL, or torch.Tensor.\")\n",
    "\n",
    "        # Ensure 3rd dimension exists for grayscale\n",
    "        if img.ndim == 2:\n",
    "            img = img[..., None]\n",
    "\n",
    "        # 2) Call Albumentations with named argument\n",
    "        out = aug(image=img)\n",
    "        # out[\"image\"] is a torch.Tensor thanks to ToTensorV2()\n",
    "        return out[\"image\"]\n",
    "    return _call\n",
    "\n",
    "# --- Define transforms (Albumentations still) ---\n",
    "train_aug = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    # Optional robustifying augs for field images:\n",
    "    # A.HorizontalFlip(p=0.5),\n",
    "    # A.ColorJitter(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # expects uint8 input\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "eval_aug = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "train_tf = albu_adapter(train_aug)\n",
    "eval_tf  = albu_adapter(eval_aug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e811aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = Folder(\n",
    "    name=\"blattodea_anomaly_detection\",\n",
    "    root=\"drive/MyDrive/Colab Notebooks/AnomalySpiders\",\n",
    "    normal_dir=\"normal\",\n",
    "    abnormal_dir=\"abnormal\",\n",
    "    train_augmentations=train_tf,\n",
    "    val_augmentations=eval_tf,\n",
    "    test_augmentations=eval_tf,\n",
    "    train_batch_size=16,\n",
    "    eval_batch_size=16,\n",
    "    num_workers=0,  # set to 0 for easier debugging; increase later\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85ca87",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b69c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b90f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff216a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158c0a99",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c29c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524a9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
